{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb108f64-4206-40b6-960b-9a0b03704c75",
   "metadata": {},
   "source": [
    "# Results\n",
    "The idea of this project is to understand and implement different reinforcement learning algorithms and compare their performance. Initially, we start with the following four algorithms:\n",
    "1. **Policy-Gradient:** Current states are the input, categorical actions the output. During training the targets are given as the action chosen, multiplied by $\\pm 1$ depending on the reward. This works due to the structure of the softmax activation function and  categorical-crossentropy loss function.\n",
    "2. **Deep-Q-Learner:** The inputs are given as the current state & and a candidate action. The output is the so-called Q-value — an estimate of the quality of an action. For action-selection we choose the action with the highest Q-value. The targets during training are formed by using the algorithm‘s *Bellman eq.*: $$Q(s_t, a_t) =r_t + \\gamma \\cdot \\mathrm{max}_a Q(s_{t+1}, a_{t+1})$$ where we denote by $\\gamma$ the so called *discount factor* which controls to which the model attends to long term behaviour.\n",
    "3. **Deep-Q-Learner with Target Network:** This is an improved version of the former algorithm in which we build the targets during learning from a second so-called *target network*. This network is not trained, but instead we copy the *training network‘s* weights $\\theta_\\mathrm{train}$ every $d$ episodes using $$\\theta_{\\mathrm{target}} \\leftarrow \\tau \\cdot \\theta_{\\mathrm{train}} + (1 - \\tau) \\cdot \\theta_{\\mathrm{target}}$$ for some $\\tau \\in [0,1]$. The intuition is, that we make convergence more stable by keeping the training targets more stable.\n",
    "4. **Double-Q-Learner:** Once again this is an iteration on the former two algorithms. The difference is that we use a modified Bellman equation $$Q_{\\mathrm{train}}(s_t, a_t)  = r_t + \\gamma \\cdot Q_{\\mathrm{target}} \\left( s_{t+1}, \\mathrm{argmax}_a Q_{\\mathrm{train}}(s_{t+1}, a_{t+1}) \\right)$$ in which we have split the action-selection (done by the training network) from the action-evaluation (done by the target network). The idea behind this is to mitigate the overestimation problem of the Q-function during training (see [*van Hasselt, et al. (2015)*](https://arxiv.org/abs/1509.06461) for details).\n",
    "\n",
    "Since the focus of this project is on learning and model-comparison, we start with a very simple environment from the [OpenAI-Gym, called CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/). However, all of the code in this repository has been kept completely generic to be able to employ our algorithms on  different environments later on. For now, we want to be able to stick to small networks and make sure all the algorithms work as expected and be able to compare the different models quickly.\n",
    "\n",
    "Before we get to the code, please use the following two cells to install the required packages and to import the necessary modules, if you want to run the later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323bac5a-704f-4ead-9ca8-c584340ab228",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium numpy plotly scipy tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b0455-9273-497b-bcd8-d710fc134195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "from agents import ModelHyperParams, PolicyGradient, QLearner, RandomAgent\n",
    "from environment_handler import CartPoleHandler\n",
    "from scripts import parameter_scan, train, TrainingHyperParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa19c02-f5d3-40c2-bb51-38221a8ee2b1",
   "metadata": {},
   "source": [
    "As an example on how to use this code base to train our version of the Q-Learner $\\textcolor{red}{\\text{choose best algo}}$ on this particular problem we could run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e43ed-7b62-4984-9fe1-109fdc526f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Adjust hyper parameters later on.\n",
    "model_params = ModelHyperParams(\n",
    "    initial_epsilon=0.5,\n",
    "    epsilon_decay_constant=0.7,\n",
    "    gamma=0.5,\n",
    "    no_hidden_layers=2,\n",
    "    units_per_hidden_layer=12,\n",
    "    kernel_initializer=\"he_uniform\"\n",
    ")\n",
    "training_params = TrainingHyperParams(\n",
    "    batch_size=256,\n",
    "    epochs=1000,\n",
    "    steps_per_epoch=2560,\n",
    "    max_buffer_size=256000\n",
    ")\n",
    "\n",
    "env_handler = CartPoleHandler()\n",
    "agent = QLearner(env_handler, model_params)\n",
    "\n",
    "result = train(env_handler, agent, training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b2938-eb9b-4d97-a99c-787be2835ee4",
   "metadata": {},
   "source": [
    "Of course getting the model to converge depends very strongly on the hyperparameters chosen. This means we need to do some hyperparameter tuning before we can get to comparing the different algorithms.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "In principle the different algorithms depend on multiple hyperparameters which a priori cannot be assumed to be independent or optimized separately. Moreover, training neural networks is not deterministic and reinforcement learning suffers even more from this problem. This means that we will have to run multiple training cycles for each hyperparameter configuration to actually have statistically significant differences. In the setting presented here we have mainly the following hyperparameters to adjust:\n",
    "* **Batch size:** Due to the very noisy data in reinforcement learning this becomes even more important\n",
    "* **Discount Factor $\\gamma$:** For the Q-Learner algorithms this governs how much we take long-term behaviour into account (see Bellman eq.).\n",
    "* **Proportion of random actions $\\varepsilon$ & its decay:** All algorithms use an $\\varepsilon$-greedy policy to balance exploration & exploitation.\n",
    "* **Model architecture:** Number of layers and nodes per layer.\n",
    "* **Optimiser & Learning Rate:** Do we choose adaptive optimisers such as *Adam*/*RMSProp* or static ones such as *SGD*. How does the *learning rate* affect convergence?\n",
    "* **Weight initialiser:** Since convergence has been shown to be somewhat flaky, clearly the random initialisation of the weights matters and this is another parameter to explore.\n",
    "* **Regularisation:** Since we might choose larger networks due to initialisation issues, we can consider counter-balancing this with regularisation (e.g. *L1*).\n",
    "* **Target network update frequency $d$ and proportion $\\tau$:** For those Q-Learners with target networks these two variables control how fast and abrupt we update the target network.\n",
    "\n",
    "Clearly, it is unfeasable even for this simple environment to scan the whole hyper-parameter space. So what can we do? We can argue that some of the parameters are in good approximation independent of the others and in fact independent of the models as they are for the most part defined by the problem:\n",
    "1. The optimal batch size is determined by how much data we need in a given batch to mitigate for the noise. While this will of course depend on the randomness of the policy and thereby the model its parameters, we suspect that these are second order effects and that the largest contribution is defined by the problem itself. This allows us to find the optimal batch size without varying the other variables.\n",
    "2. The discount factor $\\gamma$ encodes how long range behaviour affects the current reward and is therefore also mostly determined by the environment rather than the model.\n",
    "3. The remaining parameters are tougher to separate as they are interlinked.\n",
    "\n",
    "So how can we find e.g. the best batch size? We can run multiple training cycles for each batch size and count how many times the model converged to a successful state (here this is defined to be able to control the cart-pole for 200 subsequent actions before it falls). After each new training cycle we can compute a $p$-value of how likely the results come from different distributions using a *permutation test*. Once we have established the best batch size with a *p*-value of 95%, we assume this to be the best parameter setting. For demonstration purposes, we present here the code to run a parameter search, but of course it runs for a long time such that it should normally be run as a script in the background (and with more different batch sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0191d95-0c92-4201-958e-1ae2731bfa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_EPOCHS_TO_TRAIN = 10\n",
    "\n",
    "\n",
    "def get_training_params(batch_size: int) -> TrainingHyperParams:\n",
    "    return TrainingHyperParams(\n",
    "        batch_size=batch_size,\n",
    "        epochs=ALLOWED_EPOCHS_TO_TRAIN,\n",
    "        steps_per_epoch=100*1024,\n",
    "        max_buffer_size=10*100*1024\n",
    "    )\n",
    "\n",
    "\n",
    "make_agent_fct=lambda e, p: PolicyGradient(e, p, verbose=0)\n",
    "make_env_handler_fct=lambda: CartPoleHandler()\n",
    "model_params = ModelHyperParams(\n",
    "    initial_epsilon=0.5,\n",
    "    epsilon_decay_constant=0.7,\n",
    "    no_hidden_layers=2,\n",
    "    units_per_hidden_layer=24,\n",
    "    optimizer=\"adam\",\n",
    "    kernel_initializer=\"he_uniform\"\n",
    ")\n",
    "batch_sizes = [64, 256, 1024]\n",
    "hyper_param_dict = {b: (get_training_params(b), model_params) for b in batch_sizes}\n",
    "\n",
    "parameter_scan(hyper_param_dict, make_agent_fct, make_env_handler_fct, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488c089-ca51-4a1d-b874-24dd2f14d946",
   "metadata": {},
   "source": [
    "In order to not clutter the repository with these files, we have not included the scripts to run all parameter searches, but they are completely analogous to this code cell. Using this strategy for all parameters, we have found the following hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bd352-6693-4613-86de-335b8dab2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingHyperParams(\n",
    "    batch_size=256,\n",
    "    epochs=ALLOWED_EPOCHS_TO_TRAIN,\n",
    "    steps_per_epoch=400*256,\n",
    "    max_buffer_size=10*400*256\n",
    ")\n",
    "# TODO Update model params.\n",
    "model_params = ModelHyperParams(\n",
    "    initial_epsilon=0.5,\n",
    "    epsilon_decay_constant=0.7,\n",
    "    no_hidden_layers=2,\n",
    "    units_per_hidden_layer=24,\n",
    "    optimizer=\"adam\",\n",
    "    kernel_initializer=\"he_uniform\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9921790-5d0b-4574-9973-659980eceb4a",
   "metadata": {},
   "source": [
    "To make this a bit more specific, let us use this agent to control the cart-pole for some episodes and measure for how long it achieves to keep the pole stable. For comparison we then repeat the same test using the `RandomAgent`. Note, that for the presentation in this Jupyter-Notebook we have disable a graphical rendering, but it can be switched on with the `show_graphics` flag, when run through a terminal. For convenience, we show here are two sample gifs for the two agents. $\\textcolor{red}{\\text{paste in gifs}}$ **[See here](https://stackoverflow.com/questions/51527868/how-do-i-embed-a-gif-in-jupyter-notebook) on how to embed gifs in jupyter notebooks [and here](https://stackoverflow.com/questions/54100582/saving-a-video-gif-file-for-the-open-ai-gymtaxi-environment) on how to make gifs from Gym.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e5306-1580-45fe-a37b-441c36e4bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_handler = CartPoleHandler(no_of_benchmark_episodes=5, show_graphics=False)\n",
    "# TODO Adjust to env-handler, when code change is made.\n",
    "q_learner = QLearner(env_handler, model_params)\n",
    "# TODO Uncomment when ready.\n",
    "#q_learner.load_weights(\"blabla\")\n",
    "# TODO Adjust to env-handler, when code change is made.\n",
    "random_agent = RandomAgent(env_handler)\n",
    "\n",
    "random_agent_median_episode_length = environment_handler.benchmark_agent(random_agent)\n",
    "q_learner_median_episode_length = environment_handler.benchmark_agent(q_learner)\n",
    "\n",
    "print(\"Random agent median epsiode length: \", random_agent_median_episode_length)\n",
    "print(\"Q Learner median epsiode length: \", q_learner_median_episode_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38928eec-a77b-43db-8a6b-9f7eaa20741a",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "Now that all different algorithms converge, we should compare them. That is we want to find out how fast and stable they converge to a solution compared to each other. A good metric to compare how the models perform after each epoch, is to train all four models $n$ times. After each training-epoch we then let the model control the cart-pole for $m$ episodes and count how many actions each of these episodes lasted.\n",
    "\n",
    "After this, for each of the four models and each training epoch, we are left with $n \\cdot m$ numbers representing the distribution of how long the episodes lasted. We can combine this into a single median and 95%-quantiles $\\textcolor{red}{\\text{check!!!}}$ and plot them over the epochs to compare the models. Note, however, that this combines two averaging processes: One over the $n$ independently trained agents and another over the $m$ episodes we ran after a given epoch for each competing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2caf106-587c-4176-b443-142adbfdbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the following just for 2 agents each and only 5 epochs or so, so that we can create all the logic\n",
    "# and make sure it works. We can then run it again once all hyperparams are fixed.\n",
    "\n",
    "# What should go into a separate script:\n",
    "# Think about for how many epochs I want to allow models to train in the following? For sure more than 10.\n",
    "# For each model, we train it 10 times.\n",
    "# For each training run and after each epoch, we run the model 10 times (and allow for longer episodes than 200)\n",
    "# Once an agent reaches the maximum epoch number (not before), we save the results to a dictonary, where\n",
    "# The keys are [model_type, agent_id, epoch_number] and the values are the lists with results.\n",
    "# After each agent finishes we pickle this to not lose progress. \n",
    "\n",
    "# What goes here:\n",
    "# We load the script.\n",
    "# We join all the lists for the agent_id's, so that we have a dict [model_type, epoch_number] -> results_list\n",
    "# We take the quantiles: [model_type, epoch_number] -> (0.05, 0.50, 0.95)\n",
    "# Turn into pandas dataframe\n",
    "# Plot. Either separately or together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
